---
layout: post
title: Word2Vec
subtitle : NLP
tags: [Python]
author: Yeabin
comments : True
---

# Word Representation

1) Word Encoding

 * word Encoding: 단순 수치화
   	* ex) BOW, TFIDF
	* 방법론: 빈도 기반, 통계적 기반

2) Word Embedding

	* Word Embedding: 단어들의 관계, 의미적인 유사성을 갖도록 수치화
	* 방법론: 학습 기반 => 학습을 통해 단어들을 수치 벡터로 변환
	* Word Embedding 벡터는 classification 등의 특정 목적 달성을 위해 그때 마다 학습하는 방식이다.
	* 따라서, Word Embedding 벡터는 사후적으로 결정되고, 특정 목적의 용도에 한정된다.

# Word2Vec

* Word2Vec은 방대한 양의 코퍼스(임의의 문서)를 학습하여 단어들이 어떤 관계를 갖도록 벡터화 하는 기술이다.
* Word Embedding처럼 특정 목적에 맞게 벡터화하는 것이 아니라, 목적에 상관없이 범용적으로 사용할 수 있도록 벡터화 한다.
* 즉, 사전에 학습을 통해 만든 벡터를 이후에 다른 문서에 적용할 수 있다.
* Word2Vec은 단어의 주변 단어(context)를 참조하여 해당 단어를 수치화 하기 때문에 인접 단어들 간에는 단어 벡터의 유사도가 높다.
* 이 때문에 distributed representation(분포 가설)이라 한다.
* 대표적인 Word2Vec에는 CBOW와 Skip-Gram이 있다.

## CBOW(Continuous bag of word)

* CBOW는 주변 단어를 입력 받아 해당 단어가 나오도록 학습한다.
* 각 단어들은 one-hot 인코딩 형태로 입력되고 출력된다.

## Skip-Gram

* Skip-Gram은 해당 단어를 입력 받아 주변 단어가 나오도록 학습한다.
* 특정 단어에 대한 벡터를 알기 위해서 주변 단어를 모두 넣어야 하는 CBOW와 달리 단어 한 개만 입력하면 해당 단어의 벡터를 알 수 있는 Skip-Gram이 더 많이 사용된다. 
* 각 단어들은 one-hot 인코딩 형태로 입력되고 출력된다.

/* 특정 단어의 one-hot = [0,0,0,0,...,1,0,0,..,0]의 형태(길이는 vocabulary size이고 vocabulary에서 해당 단어의 index만 1인 형태) 

![Word2vec image]({{ site.baseurl }}/assets/img/CBOW_SKIP_GRAM.PNG)

